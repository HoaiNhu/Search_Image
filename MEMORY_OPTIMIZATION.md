# Memory Optimization Guide for 512MB RAM Limit

## Problem

Render free tier has **512MB RAM limit** but the CLIP model + app uses ~500MB+, causing crashes.

## Solutions Implemented

### 1. **Lazy Loading Model** (Saved ~150MB on startup)

- Model only loads when first API request comes in
- Not during server startup
- Set `LAZY_LOAD_MODEL=true` in `.env`

### 2. **On-Demand Feature Computation** (Saved ~200-300MB)

- Instead of pre-computing and caching ALL product features in memory
- Compute features only when searching
- Set `CACHE_PRODUCTS=false` in `.env`

**Trade-off:**

- ‚úÖ Uses less memory
- ‚ö†Ô∏è Slower search (1-2 seconds longer)

### 3. **Smaller Model** (Already using smallest)

- Using `openai/clip-vit-base-patch32` (~150MB)
- Alternative: Could use `sentence-transformers/clip-ViT-B-32` but similar size

### 4. **Docker Optimizations**

- Python memory optimizations
- Single worker only
- Limited concurrency to 10 requests
- Clean pip cache after install

### 5. **Memory Cleanup After Inference**

- Clear tensors after each prediction
- Release unused memory

## Configuration

### For 512MB RAM (Render Free):

```env
# Memory Optimization - REQUIRED for Render free tier
MAX_BATCH_SIZE=4
CACHE_PRODUCTS=false      # On-demand computation
LAZY_LOAD_MODEL=true      # Load only when needed
```

### For 1GB+ RAM (Paid tier):

```env
# Better performance with more memory
MAX_BATCH_SIZE=16
CACHE_PRODUCTS=true       # Pre-compute all features
LAZY_LOAD_MODEL=false     # Load immediately
```

## Expected Memory Usage

| Configuration         | Startup | Peak Usage | Notes                 |
| --------------------- | ------- | ---------- | --------------------- |
| **Free tier (512MB)** | ~100MB  | ~450MB     | On-demand computation |
| **Paid tier (1GB)**   | ~400MB  | ~800MB     | Pre-computed features |

## Testing Locally

### Check memory usage:

```bash
# During development
python -m memory_profiler run.py

# In Docker
docker stats
```

## Deployment to Render

1. **Update .env on Render Dashboard:**

   ```
   CACHE_PRODUCTS=false
   LAZY_LOAD_MODEL=true
   MAX_BATCH_SIZE=4
   ```

2. **Redeploy:**

   - Push changes to GitHub
   - Render will auto-deploy

3. **Monitor logs:**
   ```
   # Check for "Lazy loading search service components..."
   # First request will be slower (~3-5 seconds)
   # Subsequent requests: ~1-2 seconds
   ```

## Performance Expectations

### Free Tier (512MB):

- **First request**: 3-5 seconds (model loading)
- **Subsequent requests**: 1-2 seconds per search
- **Max concurrent users**: 3-5

### Paid Tier (1GB+):

- **First request**: 0.5-1 second
- **Subsequent requests**: 0.3-0.5 seconds per search
- **Max concurrent users**: 10-20

## Troubleshooting

### Still getting "Out of Memory" errors?

1. **Check logs for memory spikes:**

   ```
   Ran out of memory (used over 512MB)
   ```

2. **Reduce TOP_K:**

   ```env
   TOP_K=5  # Instead of 10
   ```

3. **Increase similarity threshold:**

   ```env
   SIMILARITY_THRESHOLD=0.7  # Instead of 0.5
   ```

   This returns fewer results = less processing

4. **Consider upgrading to paid tier:**
   - Render Starter: $7/month, 512MB ‚Üí 2GB RAM
   - Much better performance

## Alternative Lightweight Solutions

If still having issues, consider:

1. **Use OpenAI's CLIP API** (serverless)
   - No model in memory
   - Pay per request
2. **Move to Vercel/Netlify Functions**

   - Use smaller embedding models
   - Edge functions

3. **Pre-compute features offline**
   - Store in MongoDB
   - Only compare embeddings at runtime

## Upgrade Path

When ready to scale:

1. Upgrade Render plan ($7/month for 2GB RAM)
2. Enable caching: `CACHE_PRODUCTS=true`
3. Increase workers: `--workers 2` in Dockerfile
4. Add Redis for feature caching

## Current Configuration

‚úÖ Optimized for **512MB RAM** free tier
‚úÖ Model loads on first request only
‚úÖ Features computed on-demand
‚úÖ Single worker, limited concurrency
‚úÖ Memory cleaned after each request

Expected to use **400-450MB peak** - should work! üéâ
